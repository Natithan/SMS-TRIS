# See https://proceedings.mlr.press/v162/lippe22a/lippe22a.pdf#page=35
data_dir: /cw/liir_code/NoCsBack/nathan/p2/CITRIS/data_generation/id_pong/100000_train_points_paper_settings/

#causal_encoder_checkpoint: "/cw/liir_code/NoCsBack/nathan/p2/CITRIS/experiments/checkpoints/2fo052kp_CausalEncoder_pong/epoch=49-step=39050.ckpt"
#autoencoder_checkpoint: "/cw/liir_code/NoCsBack/nathan/p2/CITRIS/experiments/checkpoints/330ocogx_Autoencoder_pong/epoch=49-step=9750.ckpt"

causal_encoder_checkpoint: "/cw/liir_code/NoCsBack/nathan/p2/CITRIS/experiments/checkpoints/2ht3tf3c_CausalEncoder/epoch=199-step=390600.ckpt" # same as for non-papersettings-pong, cuz it shouldn't matter
# leave autoencoder_checkpoint empty: seed-dependent


batch_size: 1024
max_epochs: 1000

beta_classifier: 2
beta_t1: 1
lambda_reg: 0.1
lambda_sparse: 0.05
num_latents: 16
num_flows: 4

model: CITRISNF
autoregressive_prior: True
coarse_vars: False

enco_postprocessing: False